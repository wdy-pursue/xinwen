"""
æ–°é—»æ§åˆ¶å™¨ - é‡æ„ç‰ˆæœ¬
å¤„ç†æ–°é—»ç›¸å…³çš„APIè¯·æ±‚
"""

import os
import json
import glob
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from urllib.parse import unquote

logger = logging.getLogger(__name__)

class NewsController:
    """æ–°é—»æ§åˆ¶å™¨ç±»"""
    
    def __init__(self, data_dir: str = 'data'):
        self.data_dir = data_dir
        
    def get_vendor_mapping(self) -> Dict[str, str]:
        """è·å–å‚å•†åç§°æ˜ å°„"""
        return {
            'weread': 'å¾®ä¿¡è¯»ä¹¦',
            'zhihu': 'çŸ¥ä¹',
            'douban': 'è±†ç“£',
            'xiaohongshu': 'å°çº¢ä¹¦',
            'nowhots': 'ä»Šæ—¥çƒ­æ¦œ',
            'toutiao': 'ä»Šæ—¥å¤´æ¡',
            'baidu': 'ç™¾åº¦',
            'weibo': 'å¾®åš',
            'douban-group': 'è±†ç“£å°ç»„'
        }
    
    def parse_filename_info(self, filename: str) -> Dict[str, Any]:
        """
        è§£ææ–‡ä»¶åè·å–å‚å•†å’Œæ’åä¿¡æ¯
        æ–‡ä»¶åæ ¼å¼: 2025_08_11_14_weread_1_æˆ‘åœ¨ç›‘ç‹±æœåˆ‘çš„æ—¥å­_ecc99523.txt
        è¿”å›: {'vendor': 'weread', 'rank': 1, 'vendor_display': 'å¾®ä¿¡è¯»ä¹¦'}
        """
        try:
            # ç§»é™¤æ‰©å±•åå¹¶åˆ†å‰²
            name_parts = filename.replace('.txt', '').split('_')
            
            # æ–‡ä»¶åæ ¼å¼: å¹´_æœˆ_æ—¥_æ—¶_å‚å•†_æ’å_æ ‡é¢˜_ID
            if len(name_parts) >= 6:
                vendor = name_parts[4]  # å‚å•†æ ‡è¯†
                rank = int(name_parts[5]) if name_parts[5].isdigit() else 0  # æ’å
                
                # è·å–å‚å•†æ˜¾ç¤ºåç§°
                vendor_mapping = self.get_vendor_mapping()
                vendor_display = vendor_mapping.get(vendor, vendor.title())
                
                return {
                    'vendor': vendor,
                    'rank': rank,
                    'vendor_display': vendor_display
                }
        except Exception as e:
            logger.warning(f"è§£ææ–‡ä»¶åå¤±è´¥ {filename}: {e}")
        
        return {
            'vendor': 'unknown',
            'rank': 0,
            'vendor_display': 'æœªçŸ¥æ¥æº'
        }
    
    def load_articles_from_files(self) -> List[Dict[str, Any]]:
        """ä»æ–‡ä»¶ä¸­åŠ è½½æ–‡ç« æ•°æ®"""
        articles = []
        
        if not os.path.exists(self.data_dir):
            logger.warning(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return articles
        
        try:
            # è·å–æ‰€æœ‰æ–‡ç« æ–‡ä»¶
            pattern = os.path.join(self.data_dir, '*.txt')
            files = glob.glob(pattern)
            
            # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            
            for file_path in files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        article_data = json.load(f)
                    
                    # å¤„ç†æ—¶é—´æ ¼å¼
                    if 'publish_time' in article_data:
                        try:
                            # å°è¯•è§£æISOæ ¼å¼æ—¶é—´
                            dt = datetime.fromisoformat(
                                article_data['publish_time'].replace('Z', '+00:00')
                            )
                            article_data['publish_time'] = dt.strftime('%Y-%m-%d %H:%M:%S')
                            article_data['timestamp'] = dt.isoformat()
                        except:
                            pass
                    
                    # å¤„ç†æ ‡ç­¾
                    if 'tags' in article_data and isinstance(article_data['tags'], str):
                        article_data['tags'] = [
                            tag.strip() for tag in article_data['tags'].split(',') if tag.strip()
                        ]
                    
                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                    article_data['file_path'] = file_path
                    article_data['file_name'] = os.path.basename(file_path)
                    
                    # è§£ææ–‡ä»¶åè·å–å‚å•†ä¿¡æ¯
                    filename_info = self.parse_filename_info(article_data['file_name'])
                    article_data.update(filename_info)
                    
                    # å¦‚æœæ²¡æœ‰sourceå­—æ®µï¼Œä½¿ç”¨å‚å•†ä¿¡æ¯
                    if not article_data.get('source'):
                        article_data['source'] = filename_info['vendor_display']
                    
                    articles.append(article_data)
                    
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ç« æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ç« æ•°æ®å¤±è´¥: {e}")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    def filter_articles(self, articles: List[Dict[str, Any]], 
                       search: Optional[str] = None,
                       vendor: Optional[str] = None) -> List[Dict[str, Any]]:
        """ç­›é€‰æ–‡ç« """
        filtered_articles = articles.copy()
        
        # æŒ‰æœç´¢å…³é”®å­—è¿‡æ»¤
        if search:
            search_lower = search.lower()
            filtered_articles = [
                article for article in filtered_articles
                if (search_lower in (article.get('title', '') or '').lower() or
                    search_lower in (article.get('content', '') or '').lower() or
                    search_lower in (article.get('summary', '') or '').lower())
            ]
            logger.info(f"æŒ‰æœç´¢è¯ '{search}' ç­›é€‰åå‰©ä½™ {len(filtered_articles)} ç¯‡æ–‡ç« ")
        
        # æŒ‰å‚å•†è¿‡æ»¤
        if vendor:
            filtered_articles = [
                article for article in filtered_articles
                if (article.get('vendor_display', '') == vendor or 
                    article.get('source', '') == vendor)
            ]
            logger.info(f"æŒ‰å‚å•† '{vendor}' ç­›é€‰åå‰©ä½™ {len(filtered_articles)} ç¯‡æ–‡ç« ")
        
        return filtered_articles
    
    def generate_vendor_stats(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆå‚å•†ç»Ÿè®¡ä¿¡æ¯"""
        vendor_stats = {}
        
        for article in articles:
            vendor = article.get('vendor_display', 'æœªçŸ¥æ¥æº')
            
            if vendor not in vendor_stats:
                vendor_stats[vendor] = {
                    'count': 0,
                    'articles': []
                }
            
            vendor_stats[vendor]['count'] += 1
            vendor_stats[vendor]['articles'].append(article)
        
        # å¯¹æ¯ä¸ªå‚å•†çš„æ–‡ç« æŒ‰æ’åæ’åº
        for vendor_info in vendor_stats.values():
            vendor_info['articles'].sort(key=lambda x: x.get('rank', 999))
        
        return vendor_stats
    
    def get_news_list(self, page: int = 1, per_page: int = 20,
                     search: Optional[str] = None,
                     vendor: Optional[str] = None) -> Dict[str, Any]:
        """è·å–æ–°é—»åˆ—è¡¨"""
        try:
            # é™åˆ¶æ¯é¡µæ•°é‡ - å…è®¸è·å–æ›´å¤šæ–‡ç« ç”¨äºå®Œæ•´å±•ç¤º
            per_page = min(per_page, 1000)  # å¢åŠ é™åˆ¶åˆ°1000æ¡
            
            # åŠ è½½æ‰€æœ‰æ–‡ç« 
            all_articles = self.load_articles_from_files()
            
            # ç­›é€‰æ–‡ç« 
            filtered_articles = self.filter_articles(all_articles, search, vendor)
            
            # åˆ†é¡µ
            total = len(filtered_articles)
            start = (page - 1) * per_page
            end = start + per_page
            articles_page = filtered_articles[start:end]
            
            # è·å–æ‰€æœ‰å‚å•†åˆ—è¡¨ï¼ˆä»åŸå§‹æ•°æ®ä¸­è·å–ï¼Œç¡®ä¿å®Œæ•´æ€§ï¼‰
            all_vendor_stats = self.generate_vendor_stats(all_articles)
            vendors = list(all_vendor_stats.keys())
            vendors.sort()
            
            # ç”Ÿæˆç­›é€‰åçš„å‚å•†ç»Ÿè®¡
            vendor_stats = self.generate_vendor_stats(filtered_articles)
            
            return {
                'success': True,
                'data': {
                    'articles': articles_page,
                    'pagination': {
                        'page': page,
                        'per_page': per_page,
                        'total': total,
                        'pages': (total + per_page - 1) // per_page
                    },
                    'vendors': vendors,  # æ‰€æœ‰å‚å•†åˆ—è¡¨
                    'sources': vendors,  # å…¼å®¹æ—§å­—æ®µ
                    'vendor_stats': vendor_stats,  # å½“å‰ç­›é€‰ç»“æœçš„å‚å•†ç»Ÿè®¡
                    'timestamp': datetime.now().isoformat(),
                    'filters': {
                        'search': search,
                        'vendor': vendor
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {str(e)}',
                'error': str(e)
            }
    
    def get_news_detail(self, file_name: str) -> Dict[str, Any]:
        """è·å–æ–°é—»è¯¦æƒ…"""
        try:
            # URLè§£ç æ–‡ä»¶åä»¥å¤„ç†ä¸­æ–‡å­—ç¬¦
            decoded_file_name = unquote(file_name)
            file_path = os.path.join(self.data_dir, decoded_file_name)
            
            if not os.path.exists(file_path):
                return {
                    'success': False,
                    'message': 'æ–‡ç« ä¸å­˜åœ¨',
                    'error': 'Article not found'
                }
            
            with open(file_path, 'r', encoding='utf-8') as f:
                article_data = json.load(f)
            
            # å¤„ç†æ—¶é—´æ ¼å¼
            if 'publish_time' in article_data:
                try:
                    dt = datetime.fromisoformat(
                        article_data['publish_time'].replace('Z', '+00:00')
                    )
                    article_data['publish_time'] = dt.strftime('%Y-%m-%d %H:%M:%S')
                    article_data['timestamp'] = dt.isoformat()
                except:
                    pass
            
            # å¤„ç†æ ‡ç­¾
            if 'tags' in article_data and isinstance(article_data['tags'], str):
                article_data['tags'] = [
                    tag.strip() for tag in article_data['tags'].split(',') if tag.strip()
                ]
            
            # è§£ææ–‡ä»¶åä¿¡æ¯
            filename_info = self.parse_filename_info(file_name)
            article_data.update(filename_info)
            
            return {
                'success': True,
                'data': article_data
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–°é—»è¯¦æƒ…å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'è·å–æ–°é—»è¯¦æƒ…å¤±è´¥: {str(e)}',
                'error': str(e)
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            articles = self.load_articles_from_files()
            
            # æŒ‰å‚å•†ç»Ÿè®¡
            vendor_stats = {}
            for article in articles:
                vendor = article.get('vendor_display', 'æœªçŸ¥æ¥æº')
                vendor_stats[vendor] = vendor_stats.get(vendor, 0) + 1
            
            # æŒ‰æ—¥æœŸç»Ÿè®¡ï¼ˆæœ€è¿‘7å¤©ï¼‰
            date_stats = {}
            for article in articles:
                try:
                    publish_time = article.get('publish_time', '')
                    if publish_time:
                        date = datetime.fromisoformat(
                            publish_time.replace('Z', '+00:00')
                        ).strftime('%Y-%m-%d')
                        date_stats[date] = date_stats.get(date, 0) + 1
                except:
                    pass
            
            return {
                'success': True,
                'data': {
                    'total_articles': len(articles),
                    'vendor_stats': vendor_stats,
                    'source_stats': vendor_stats,  # å…¼å®¹æ—§æ¥å£
                    'date_stats': dict(sorted(date_stats.items(), reverse=True)[:7]),
                    'last_updated': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}',
                'error': str(e)
            }
    
    def get_news_by_keyword(self, keyword: str) -> Dict[str, Any]:
        """æ ¹æ®å…³é”®è¯è·å–ç›¸å…³æ–°é—» - ä½¿ç”¨ä¸é¦–é¡µæœç´¢ç›¸åŒçš„é€»è¾‘"""
        try:
            # URLè§£ç å…³é”®è¯ä»¥å¤„ç†ä¸­æ–‡å­—ç¬¦
            decoded_keyword = unquote(keyword)
            # åŠ è½½æ‰€æœ‰æ–‡ç« 
            articles = self.load_articles_from_files()
            logger.info(f"ğŸ” æœç´¢å…³é”®è¯: '{decoded_keyword}', æ€»æ–‡ç« æ•°: {len(articles)}")
            
            # ä½¿ç”¨ä¸ filter_articles ç›¸åŒçš„é€»è¾‘è¿›è¡Œæœç´¢
            related_articles = self.filter_articles(articles, search=decoded_keyword)
            
            logger.info(f"å…³é”®è¯ '{decoded_keyword}' æ‰¾åˆ° {len(related_articles)} ç¯‡æ–‡ç« ")
            
            # æŒ‰æ’åæ’åº
            related_articles.sort(key=lambda x: x.get('rank', 999))
            
            return {
                'success': True,
                'data': {
                    'keyword': decoded_keyword,
                    'total': len(related_articles),
                    'articles': related_articles
                }
            }
            
        except Exception as e:
            logger.error(f"æ ¹æ®å…³é”®è¯è·å–æ–°é—»å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
